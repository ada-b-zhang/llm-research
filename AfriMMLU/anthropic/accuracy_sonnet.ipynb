{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "This code processes TSV files containing LLM (Language Model) outputs to calculate the accuracy of predictions. The results are saved into new CSV files with the added accuracy information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Each TSV File\n",
    "For each TSV file:\n",
    "- The file is read into a Pandas DataFrame using `pd.read_csv()`.\n",
    "- The code checks for the presence of expected columns (`'question'`, `'choices'`, `'answer'`, `'subject'`, `'llm'`).\n",
    "- Accuracy is calculated by comparing the actual answer (`'answer'`) with the predictions extracted from the `'llm'` column using regex.\n",
    "- An accuracy column is added to the DataFrame.\n",
    "- The updated DataFrame is saved as a new CSV file, named based on the language (derived from the original file name without the `.tsv` extension)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: orm.csv\n",
      "Processed: xho.csv\n",
      "Processed: sot.csv\n",
      "Processed: swa.csv\n",
      "Processed: sna.csv\n",
      "Processed: fra.csv\n",
      "Processed: zul.csv\n",
      "Processed: lin.csv\n",
      "Processed: ibo.csv\n",
      "Processed: twi.csv\n",
      "Processed: amh.csv\n",
      "Processed: eng.csv\n",
      "Processed: ewe.csv\n",
      "Processed: wol.csv\n",
      "Processed: hau.csv\n",
      "Processed: yor.csv\n",
      "Processed: lug.csv\n",
      "Processed: kin.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def extract_llm_answer(value):\n",
    "    \"\"\"Extract the first letter (answer) from a TextBlock representation.\"\"\"\n",
    "    try:\n",
    "        # Use regex to capture the first letter (answer) from TextBlock(text='B)About $3k', type='text')\n",
    "        match = re.search(r\"TextBlock\\(text=['\\\"]([A-Da-d])\\)\", value)\n",
    "        return match.group(1) if match else \"\"\n",
    "    except TypeError:\n",
    "        return \"\"\n",
    "\n",
    "input_path = \"sonnet35/\"\n",
    "output_path = \"sonnet35/accuracy/\"\n",
    "\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "for file_name in os.listdir(input_path):\n",
    "    if file_name.endswith(\".csv\"):\n",
    "        try:\n",
    "            file_path = os.path.join(input_path, file_name)\n",
    "            \n",
    "            # Read the file with tab as delimiter\n",
    "            df = pd.read_csv(file_path, sep='\\t', on_bad_lines=\"skip\")\n",
    "\n",
    "            # Normalize column names (strip spaces and lowercase)\n",
    "            df.columns = df.columns.str.strip().str.lower()\n",
    "\n",
    "            # Ensure required columns exist\n",
    "            if 'answer' not in df.columns or 'llm' not in df.columns:\n",
    "                print(f\"Skipping {file_name}: Missing required columns.\")\n",
    "                continue\n",
    "\n",
    "            # Extract llm_answer from the llm column\n",
    "            df['llm_answer'] = df['llm'].apply(extract_llm_answer)\n",
    "\n",
    "            # Add accuracy column as 0 or 1\n",
    "            df['accuracy'] = (df['llm_answer'] == df['answer']).astype(int)\n",
    "\n",
    "            # Save updated file\n",
    "            new_file_name = file_name.split(\".\")[0] + \"_accuracy.csv\"\n",
    "            new_file_path = os.path.join(output_path, new_file_name)\n",
    "            df.to_csv(new_file_path, index=False)\n",
    "\n",
    "            print(f\"Processed: {file_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_name}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Purpose of the Sanity Check\n",
    "The sanity check ensures that the calculated mean accuracies from the LLaMA model outputs align closely with the known results from AfriMMLU. If there are significant discrepancies, it could indicate:\n",
    "- Issues in data processing or accuracy calculation.\n",
    "- Differences in evaluation methods or dataset versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_accuracy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>language</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>amh</th>\n",
       "      <td>0.628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eng</th>\n",
       "      <td>0.748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ewe</th>\n",
       "      <td>0.370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fra</th>\n",
       "      <td>0.712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hau</th>\n",
       "      <td>0.480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ibo</th>\n",
       "      <td>0.498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kin</th>\n",
       "      <td>0.496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lin</th>\n",
       "      <td>0.480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lug</th>\n",
       "      <td>0.456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>orm</th>\n",
       "      <td>0.518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sna</th>\n",
       "      <td>0.516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sot</th>\n",
       "      <td>0.520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>swa</th>\n",
       "      <td>0.638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>twi</th>\n",
       "      <td>0.394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wol</th>\n",
       "      <td>0.356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xho</th>\n",
       "      <td>0.538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yor</th>\n",
       "      <td>0.506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zul</th>\n",
       "      <td>0.536</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          mean_accuracy\n",
       "language               \n",
       "amh               0.628\n",
       "eng               0.748\n",
       "ewe               0.370\n",
       "fra               0.712\n",
       "hau               0.480\n",
       "ibo               0.498\n",
       "kin               0.496\n",
       "lin               0.480\n",
       "lug               0.456\n",
       "orm               0.518\n",
       "sna               0.516\n",
       "sot               0.520\n",
       "swa               0.638\n",
       "twi               0.394\n",
       "wol               0.356\n",
       "xho               0.538\n",
       "yor               0.506\n",
       "zul               0.536"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List all the accuracy CSV files\n",
    "accuracy_files = [f for f in os.listdir(output_path) if f.endswith('_accuracy.csv')]\n",
    "\n",
    "# Initialize a list to hold individual DataFrames\n",
    "dfs = []\n",
    "\n",
    "# Loop through each CSV file and load it into a DataFrame\n",
    "for accuracy_file in accuracy_files:\n",
    "    file_path = os.path.join(output_path, accuracy_file)\n",
    "    try:\n",
    "        # Load the CSV into a DataFrame\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Add a column to the DataFrame for the file name (language)\n",
    "        df['language'] = accuracy_file.replace('_accuracy.csv', '')  # Remove '_accuracy.csv' to get the language name\n",
    "        \n",
    "        # Add the DataFrame to the list\n",
    "        dfs.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {accuracy_file}: {e}\")\n",
    "\n",
    "# Concatenate all DataFrames into one\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "grouped_mean_accuracy = combined_df.groupby(['language']).agg(\n",
    "    mean_accuracy=('accuracy', 'mean'))\n",
    "grouped_mean_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](../paper_results.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match for amh: 62.8% (Difference: 0.0%)\n",
      "Match for eng: 74.8% (Difference: 0.0%)\n",
      "Match for ewe: 37.0% (Difference: 0.0%)\n",
      "Match for fra: 71.2% (Difference: 0.0%)\n",
      "Match for hau: 48.0% (Difference: 0.0%)\n",
      "Match for ibo: 49.8% (Difference: 0.0%)\n",
      "Match for kin: 49.6% (Difference: 0.0%)\n",
      "Match for lin: 48.0% (Difference: 0.0%)\n",
      "Match for lug: 45.6% (Difference: 0.0%)\n",
      "Match for orm: 51.8% (Difference: 0.0%)\n",
      "Match for sna: 51.6% (Difference: 0.0%)\n",
      "Match for sot: 52.0% (Difference: 0.0%)\n",
      "Match for swa: 63.8% (Difference: 0.0%)\n",
      "Match for twi: 39.4% (Difference: 0.0%)\n",
      "Match for wol: 35.6% (Difference: 0.0%)\n",
      "Match for xho: 53.8% (Difference: 0.0%)\n",
      "Match for yor: 50.6% (Difference: 0.0%)\n",
      "Match for zul: 53.6% (Difference: 0.0%)\n"
     ]
    }
   ],
   "source": [
    "anthropic_results = {\n",
    "    'amh': 62.8,   # Amharic\n",
    "    'eng': 74.8,   # English\n",
    "    'ewe': 37.0,   # Ewe\n",
    "    'fra': 71.2,   # French\n",
    "    'hau': 48.0,   # Hausa\n",
    "    'ibo': 49.8,   # Igbo\n",
    "    'kin': 49.6,   # Kinyarwanda\n",
    "    'lin': 48.0,   # Lingala\n",
    "    'lug': 45.6,   # Luganda\n",
    "    'orm': 51.8,   # Oromo\n",
    "    'sna': 51.6,   # Shona\n",
    "    'sot': 52.0,   # Southern Sotho\n",
    "    'swa': 63.8,   # Swahili\n",
    "    'twi': 39.4,   # Twi\n",
    "    'wol': 35.6,   # Wolof\n",
    "    'xho': 53.8,   # Xhosa\n",
    "    'yor': 50.6,   # Yoruba\n",
    "    'zul': 53.6    # Zulu\n",
    "}\n",
    "\n",
    "for language, expected_accuracy in anthropic_results.items():\n",
    "    try:\n",
    "        actual_accuracy = grouped_mean_accuracy.loc[language, 'mean_accuracy'] * 100\n",
    "        difference = abs(actual_accuracy - expected_accuracy)\n",
    "        \n",
    "        if difference > 1:\n",
    "            print(f\"Significant difference for {language}: Expected {expected_accuracy}%, but got {actual_accuracy:.1f}%. Difference: {difference:.1f}\")\n",
    "        else:\n",
    "            print(f\"Match for {language}: {actual_accuracy:.1f}% (Difference: {difference:.1f}%)\")\n",
    "    except KeyError:\n",
    "        print(f\"KeyError: '{language}' not found in the DataFrame.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "practicum1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
