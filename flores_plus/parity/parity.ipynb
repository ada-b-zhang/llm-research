{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FLORES+\n",
    "\n",
    "[FLORES+ on Hugging Face](https://huggingface.co/datasets/openlanguagedata/flores_plus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2091720af98459091a7ccbfd7ec2bb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/218 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4b6630ac6534802835a892a6b592a4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/213 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b54a0e43c0484c81b89313638cefcfaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/218 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c6cff7b41fa4fd69f13e3572899e194",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/213 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af38669182144f32ad9d9be791c472b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/218 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db3b366a09e44ed9ab2e46cf42d14a1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/213 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd68d2d3afff4e578166ca755c768fc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/218 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "051d40463d9e4902ac05a6f9ba3752c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/213 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load FLORES+ \n",
    "flores_plus = load_dataset(\"openlanguagedata/flores_plus\")\n",
    "\n",
    "# Load FLORES dev split and convert to Pandas DataFrame\n",
    "flores_plus_dev = load_dataset(\"openlanguagedata/flores_plus\", split='dev').to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(217346, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>iso_639_3</th>\n",
       "      <th>language</th>\n",
       "      <th>glottocode</th>\n",
       "      <th>text</th>\n",
       "      <th>url</th>\n",
       "      <th>domain</th>\n",
       "      <th>topic</th>\n",
       "      <th>has_image</th>\n",
       "      <th>has_hyperlink</th>\n",
       "      <th>last_updated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ace</td>\n",
       "      <td>Arab</td>\n",
       "      <td>achi1257</td>\n",
       "      <td>يق أورو سنين، اوق علمون دري فکولتس کدوکترن يون...</td>\n",
       "      <td>https://en.wikinews.org/wiki/Scientists_say_ne...</td>\n",
       "      <td>wikinews</td>\n",
       "      <td>health</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ace</td>\n",
       "      <td>Arab</td>\n",
       "      <td>achi1257</td>\n",
       "      <td>ڤنليتي اوتام خن اترا ڽو موڠکين محسى ديتيکسي فو...</td>\n",
       "      <td>https://en.wikinews.org/wiki/Scientists_say_ne...</td>\n",
       "      <td>wikinews</td>\n",
       "      <td>health</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id iso_639_3 language glottocode  \\\n",
       "0   0       ace     Arab   achi1257   \n",
       "1   1       ace     Arab   achi1257   \n",
       "\n",
       "                                                text  \\\n",
       "0  يق أورو سنين، اوق علمون دري فکولتس کدوکترن يون...   \n",
       "1  ڤنليتي اوتام خن اترا ڽو موڠکين محسى ديتيکسي فو...   \n",
       "\n",
       "                                                 url    domain   topic  \\\n",
       "0  https://en.wikinews.org/wiki/Scientists_say_ne...  wikinews  health   \n",
       "1  https://en.wikinews.org/wiki/Scientists_say_ne...  wikinews  health   \n",
       "\n",
       "  has_image has_hyperlink last_updated  \n",
       "0       yes           yes          1.0  \n",
       "1       yes           yes          1.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flores_plus_dev = flores_plus_dev.rename(columns={'iso_15924':'language'})\n",
    "print(flores_plus_dev.shape)\n",
    "flores_plus_dev.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parity(text, tokenizer):\n",
    "    \"\"\" \n",
    "    Parity = # tokens in tokenized document / # number of words in original document \n",
    "\n",
    "    Note: The difference between parity and fertility is that partiy is at the DOCUMENT\n",
    "          level, whereas fertility is at the CORPUS level. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        - text (str): Text for which you want to calculate parity score\n",
    "        - tokenizer (tokenizer): Tokenizer you'd like to use \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        - parity (float): Parity score \n",
    "        - tokenized (): Tokenized text \n",
    "    \"\"\"\n",
    "    tokenized = tokenizer.tokenize(text) # Note: Transformers typically doesn't remove stopwords \n",
    "    num_words = len(text.split()) \n",
    "    parity = len(tokenized) / num_words\n",
    "    return parity, tokenized\n",
    "\n",
    "def get_parities(dataset, model='microsoft/Phi-3.5-mini-instruct', name_for_csv='output'):\n",
    "    \"\"\" \n",
    "    Get the parity score and tokens for each document (row) in the FLORES+ dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        - dataset (pd.DataFrame): Dataset for which you want to calculate parity scores \n",
    "        - model (str): Model for tokenization\n",
    "        - name_for_csv (str): Name for csv \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        - dataset_copy (pd.DataFrame): DataFrame of results\n",
    "        - Makes dataframe of parity score and tokens for each language and outputs to csv file\n",
    "          in current directory\n",
    "    \n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "    parity_scores = []\n",
    "    tokens = []\n",
    "    for row_index in range(len(dataset)):\n",
    "        text = dataset.loc[row_index, 'text']\n",
    "        parity_score, tokenized = parity(text, tokenizer)\n",
    "\n",
    "        parity_scores.append(parity_score)\n",
    "        tokens.append(tokenized)\n",
    "\n",
    "    dataset_copy = dataset.copy()\n",
    "    dataset_copy['parity'] = pd.Series(parity_scores)\n",
    "    dataset_copy['tokens'] = pd.Series(tokens)\n",
    "    dataset_copy.to_csv(f'{name_for_csv}.csv', index=True)\n",
    "    return dataset_copy \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>iso_639_3</th>\n",
       "      <th>language</th>\n",
       "      <th>glottocode</th>\n",
       "      <th>text</th>\n",
       "      <th>url</th>\n",
       "      <th>domain</th>\n",
       "      <th>topic</th>\n",
       "      <th>has_image</th>\n",
       "      <th>has_hyperlink</th>\n",
       "      <th>last_updated</th>\n",
       "      <th>parity</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ace</td>\n",
       "      <td>Arab</td>\n",
       "      <td>achi1257</td>\n",
       "      <td>يق أورو سنين، اوق علمون دري فکولتس کدوکترن يون...</td>\n",
       "      <td>https://en.wikinews.org/wiki/Scientists_say_ne...</td>\n",
       "      <td>wikinews</td>\n",
       "      <td>health</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.977778</td>\n",
       "      <td>[▁, ي, ق, ▁, أ, و, ر, و, ▁, س, ن, ي, ن, ،, ▁, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ace</td>\n",
       "      <td>Arab</td>\n",
       "      <td>achi1257</td>\n",
       "      <td>ڤنليتي اوتام خن اترا ڽو موڠکين محسى ديتيکسي فو...</td>\n",
       "      <td>https://en.wikinews.org/wiki/Scientists_say_ne...</td>\n",
       "      <td>wikinews</td>\n",
       "      <td>health</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.615385</td>\n",
       "      <td>[▁, &lt;0xDA&gt;, &lt;0xA4&gt;, ن, ل, ي, ت, ي, ▁, ا, و, ت,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>ace</td>\n",
       "      <td>Arab</td>\n",
       "      <td>achi1257</td>\n",
       "      <td>جاس ۳۹سي ݢريڤين مڤوق لندسن ڤاچو ليڠک ڤوه ۹:۳۰ ...</td>\n",
       "      <td>https://en.wikinews.org/wiki/Fighter_jet_crash...</td>\n",
       "      <td>wikinews</td>\n",
       "      <td>accident</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.272727</td>\n",
       "      <td>[▁, ج, ا, س, ▁, &lt;0xDB&gt;, &lt;0xB3&gt;, &lt;0xDB&gt;, &lt;0xB9&gt;...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id iso_639_3 language glottocode  \\\n",
       "0   0       ace     Arab   achi1257   \n",
       "1   1       ace     Arab   achi1257   \n",
       "2   2       ace     Arab   achi1257   \n",
       "\n",
       "                                                text  \\\n",
       "0  يق أورو سنين، اوق علمون دري فکولتس کدوکترن يون...   \n",
       "1  ڤنليتي اوتام خن اترا ڽو موڠکين محسى ديتيکسي فو...   \n",
       "2  جاس ۳۹سي ݢريڤين مڤوق لندسن ڤاچو ليڠک ڤوه ۹:۳۰ ...   \n",
       "\n",
       "                                                 url    domain     topic  \\\n",
       "0  https://en.wikinews.org/wiki/Scientists_say_ne...  wikinews    health   \n",
       "1  https://en.wikinews.org/wiki/Scientists_say_ne...  wikinews    health   \n",
       "2  https://en.wikinews.org/wiki/Fighter_jet_crash...  wikinews  accident   \n",
       "\n",
       "  has_image has_hyperlink last_updated    parity  \\\n",
       "0       yes           yes          1.0  5.977778   \n",
       "1       yes           yes          1.0  5.615385   \n",
       "2       yes           yes          1.0  6.272727   \n",
       "\n",
       "                                              tokens  \n",
       "0  [▁, ي, ق, ▁, أ, و, ر, و, ▁, س, ن, ي, ن, ،, ▁, ...  \n",
       "1  [▁, <0xDA>, <0xA4>, ن, ل, ي, ت, ي, ▁, ا, و, ت,...  \n",
       "2  [▁, ج, ا, س, ▁, <0xDB>, <0xB3>, <0xDB>, <0xB9>...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test \n",
    "subset = flores_plus_dev.iloc[:3, ] # First 3 documents (rows)\n",
    "testing = get_parities(subset)\n",
    "testing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Models:  11%|█         | 1/9 [00:21<02:51, 21.41s/model]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with google/flan-t5-xxl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Models:  22%|██▏       | 2/9 [00:45<02:39, 22.75s/model]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with bigscience/mt0-xxl-mt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Models:  33%|███▎      | 3/9 [01:08<02:17, 22.98s/model]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with CohereForAI/aya-101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Models:  44%|████▍     | 4/9 [01:29<01:51, 22.27s/model]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with bigscience/bloomz-7b1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Models:  56%|█████▌    | 5/9 [01:47<01:22, 20.55s/model]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with microsoft/Phi-3.5-mini-instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Models:  67%|██████▋   | 6/9 [02:12<01:06, 22.26s/model]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with neulab/Pangea-7B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Models:  78%|███████▊  | 7/9 [02:28<00:40, 20.17s/model]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with google/gemma-7b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Models:  89%|████████▉ | 8/9 [02:44<00:18, 18.74s/model]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with google/gemma-2-9b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Models: 100%|██████████| 9/9 [03:07<00:00, 20.81s/model]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with meta-llama/Llama-3.2-1B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "models = [\"google/flan-t5-xxl\",\n",
    "          \"bigscience/mt0-xxl-mt\",\n",
    "          \"CohereForAI/aya-101\",\n",
    "          \"bigscience/bloomz-7b1\",\n",
    "          \"microsoft/Phi-3.5-mini-instruct\",\n",
    "          \"neulab/Pangea-7B\",\n",
    "          \"google/gemma-7b\",\n",
    "          \"google/gemma-2-9b\",\n",
    "          \"meta-llama/Llama-3.2-1B-Instruct\"]\n",
    "names_for_csv = ['flan-t5-xxl',\n",
    "                 'mt0-xxl-mt',\n",
    "                 'aya-101',\n",
    "                 'bloomz-7b1',\n",
    "                 'Phi-3.5-mini-instruct',\n",
    "                 'Pangea-7B',\n",
    "                 'gemma-7b',\n",
    "                 'gemma-2-9b', \n",
    "                 'Llama-3.2-1B-Instruct']\n",
    "\n",
    "for i in tqdm(range(len(models)), desc=\"Processing Models\", unit=\"model\"):\n",
    "    get_parities(flores_plus_dev, model=models[i], name_for_csv=names_for_csv[i])\n",
    "    print(f\"Done with {models[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = []\n",
    "for model in names_for_csv:\n",
    "    path = f'{model}.csv'\n",
    "    dataframe = pd.read_csv(path)\n",
    "    dataframe['model'] = model\n",
    "    dataframes.append(dataframe)\n",
    "combined = pd.concat(dataframes, axis=0, ignore_index=True)\n",
    "combined.to_csv(f'all_model_parities.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>iso_639_3</th>\n",
       "      <th>language</th>\n",
       "      <th>glottocode</th>\n",
       "      <th>text</th>\n",
       "      <th>url</th>\n",
       "      <th>domain</th>\n",
       "      <th>topic</th>\n",
       "      <th>has_image</th>\n",
       "      <th>has_hyperlink</th>\n",
       "      <th>last_updated</th>\n",
       "      <th>parity</th>\n",
       "      <th>tokens</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ace</td>\n",
       "      <td>Arab</td>\n",
       "      <td>achi1257</td>\n",
       "      <td>يق أورو سنين، اوق علمون دري فکولتس کدوکترن يون...</td>\n",
       "      <td>https://en.wikinews.org/wiki/Scientists_say_ne...</td>\n",
       "      <td>wikinews</td>\n",
       "      <td>health</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.155556</td>\n",
       "      <td>['▁', 'يق', '▁', 'أورو', '▁', 'سنين،', '▁', 'ا...</td>\n",
       "      <td>flan-t5-xxl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>ace</td>\n",
       "      <td>Arab</td>\n",
       "      <td>achi1257</td>\n",
       "      <td>ڤنليتي اوتام خن اترا ڽو موڠکين محسى ديتيکسي فو...</td>\n",
       "      <td>https://en.wikinews.org/wiki/Scientists_say_ne...</td>\n",
       "      <td>wikinews</td>\n",
       "      <td>health</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.307692</td>\n",
       "      <td>['▁', 'ڤنليتي', '▁', 'اوتام', '▁', 'خن', '▁', ...</td>\n",
       "      <td>flan-t5-xxl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>ace</td>\n",
       "      <td>Arab</td>\n",
       "      <td>achi1257</td>\n",
       "      <td>جاس ۳۹سي ݢريڤين مڤوق لندسن ڤاچو ليڠک ڤوه ۹:۳۰ ...</td>\n",
       "      <td>https://en.wikinews.org/wiki/Fighter_jet_crash...</td>\n",
       "      <td>wikinews</td>\n",
       "      <td>accident</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.181818</td>\n",
       "      <td>['▁', 'جاس', '▁', '۳۹سي', '▁', 'ݢريڤين', '▁', ...</td>\n",
       "      <td>flan-t5-xxl</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  id iso_639_3 language glottocode  \\\n",
       "0           0   0       ace     Arab   achi1257   \n",
       "1           1   1       ace     Arab   achi1257   \n",
       "2           2   2       ace     Arab   achi1257   \n",
       "\n",
       "                                                text  \\\n",
       "0  يق أورو سنين، اوق علمون دري فکولتس کدوکترن يون...   \n",
       "1  ڤنليتي اوتام خن اترا ڽو موڠکين محسى ديتيکسي فو...   \n",
       "2  جاس ۳۹سي ݢريڤين مڤوق لندسن ڤاچو ليڠک ڤوه ۹:۳۰ ...   \n",
       "\n",
       "                                                 url    domain     topic  \\\n",
       "0  https://en.wikinews.org/wiki/Scientists_say_ne...  wikinews    health   \n",
       "1  https://en.wikinews.org/wiki/Scientists_say_ne...  wikinews    health   \n",
       "2  https://en.wikinews.org/wiki/Fighter_jet_crash...  wikinews  accident   \n",
       "\n",
       "  has_image has_hyperlink  last_updated    parity  \\\n",
       "0       yes           yes           1.0  2.155556   \n",
       "1       yes           yes           1.0  2.307692   \n",
       "2       yes           yes           1.0  2.181818   \n",
       "\n",
       "                                              tokens        model  \n",
       "0  ['▁', 'يق', '▁', 'أورو', '▁', 'سنين،', '▁', 'ا...  flan-t5-xxl  \n",
       "1  ['▁', 'ڤنليتي', '▁', 'اوتام', '▁', 'خن', '▁', ...  flan-t5-xxl  \n",
       "2  ['▁', 'جاس', '▁', '۳۹سي', '▁', 'ݢريڤين', '▁', ...  flan-t5-xxl  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
