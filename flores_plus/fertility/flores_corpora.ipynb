{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FLORES+\n",
    "\n",
    "[FLORES+ on HuggingFace](https://huggingface.co/datasets/openlanguagedata/flores_plus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d27b3e877c8943d28c43f97aaaed100e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/218 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cac17dcfdd2f41978e9d132457a51f2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/213 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8040d67d720a4bf5acb363c45c680dda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/218 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d45da8f1f09c455384d374b908e0d72f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/213 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4adbd33f800c4d5c80587207132190ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/218 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "229b539a6cd2476a9658cc553599ad1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/213 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ab7b99d7dcc43cdba726fa83733d829",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/218 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bf12e8f260d477c98322f5d76d606de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/213 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load FLORES+ \n",
    "flores_plus = load_dataset(\"openlanguagedata/flores_plus\")\n",
    "\n",
    "# Load FLORES dev split and convert to Pandas DataFrame\n",
    "flores_plus_dev = load_dataset(\"openlanguagedata/flores_plus\", split='dev').to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(217346, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>iso_639_3</th>\n",
       "      <th>language</th>\n",
       "      <th>glottocode</th>\n",
       "      <th>text</th>\n",
       "      <th>url</th>\n",
       "      <th>domain</th>\n",
       "      <th>topic</th>\n",
       "      <th>has_image</th>\n",
       "      <th>has_hyperlink</th>\n",
       "      <th>last_updated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ace</td>\n",
       "      <td>Arab</td>\n",
       "      <td>achi1257</td>\n",
       "      <td>يق أورو سنين، اوق علمون دري فکولتس کدوکترن يون...</td>\n",
       "      <td>https://en.wikinews.org/wiki/Scientists_say_ne...</td>\n",
       "      <td>wikinews</td>\n",
       "      <td>health</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ace</td>\n",
       "      <td>Arab</td>\n",
       "      <td>achi1257</td>\n",
       "      <td>ڤنليتي اوتام خن اترا ڽو موڠکين محسى ديتيکسي فو...</td>\n",
       "      <td>https://en.wikinews.org/wiki/Scientists_say_ne...</td>\n",
       "      <td>wikinews</td>\n",
       "      <td>health</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id iso_639_3 language glottocode  \\\n",
       "0   0       ace     Arab   achi1257   \n",
       "1   1       ace     Arab   achi1257   \n",
       "\n",
       "                                                text  \\\n",
       "0  يق أورو سنين، اوق علمون دري فکولتس کدوکترن يون...   \n",
       "1  ڤنليتي اوتام خن اترا ڽو موڠکين محسى ديتيکسي فو...   \n",
       "\n",
       "                                                 url    domain   topic  \\\n",
       "0  https://en.wikinews.org/wiki/Scientists_say_ne...  wikinews  health   \n",
       "1  https://en.wikinews.org/wiki/Scientists_say_ne...  wikinews  health   \n",
       "\n",
       "  has_image has_hyperlink last_updated  \n",
       "0       yes           yes          1.0  \n",
       "1       yes           yes          1.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flores_plus_dev = flores_plus_dev.rename(columns={'iso_15924':'language'})\n",
    "print(flores_plus_dev.shape)\n",
    "flores_plus_dev.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 31 in the FLORES+ dataset:\n",
      "['Arab' 'Latn' 'Ethi' 'Beng' 'Deva' 'Cyrl' 'Tibt' 'Hans' 'Hant' 'Grek'\n",
      " 'Gujr' 'Hebr' 'Armn' 'Jpan' 'Knda' 'Geor' 'Khmr' 'Hang' 'Laoo' 'Mlym'\n",
      " 'Mtei' 'Mymr' 'Nkoo' 'Orya' 'Guru' 'Olck' 'Sinh' 'Taml' 'Tfng' 'Telu'\n",
      " 'Thai']\n"
     ]
    }
   ],
   "source": [
    "# Get unique languages\n",
    "print(f\"There are {len(flores_plus_dev['language'].unique())} in the FLORES+ dataset:\")\n",
    "print(flores_plus_dev['language'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = list(flores_plus_dev['language'].unique())\n",
    "for i in range(len(languages)):\n",
    "    language = languages[i]\n",
    "    text = flores_plus_dev[flores_plus_dev['language']==language]['text']\n",
    "    joined = \" \".join(text)\n",
    "    with open(f\"language_corpora/{language}_corpus.txt\", \"w\") as f:\n",
    "        f.write(joined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fertility(text, tokenizer):\n",
    "    tokenized = tokenizer.tokenize(text) # Note: Transformers typically doesn't remove stopwords \n",
    "    num_words = len(text.split())\n",
    "\n",
    "    fertility = len(tokenized) / num_words\n",
    "    return fertility, tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fertilities(model='microsoft/Phi-3.5-mini-instruct', name_for_csv='output'):\n",
    "    \"\"\" \n",
    "    Get the fertility score and tokens for each language in the FLORES+ dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        - model (str): Model for tokenization\n",
    "        - name_for_csv (str): Name for csv \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None. Makes dataframe of fertility score and tokens for each language\n",
    "    and outputs to csv file. \n",
    "    \n",
    "    \"\"\"\n",
    "    tokenizer=AutoTokenizer.from_pretrained(model)\n",
    "    directory_path = \"language_corpora\"\n",
    "    languages = []\n",
    "    fertility_scores = []\n",
    "    tokens = []\n",
    "    for file in os.listdir(directory_path):\n",
    "        language = file.rstrip('_corpus.txt')\n",
    "        file_path = os.path.join(directory_path, file)  \n",
    "        with open(file_path, \"r\") as corpus:\n",
    "            text = corpus.read()\n",
    "        \n",
    "        fertility_score, tokenized = fertility(text, tokenizer)\n",
    "\n",
    "        languages.append(language)\n",
    "        fertility_scores.append(fertility_score)\n",
    "        tokens.append(tokenized)\n",
    "\n",
    "\n",
    "        df = pd.DataFrame({'language': pd.Series(languages),\n",
    "                        'fertility': pd.Series(fertility_scores),\n",
    "                        'tokens': pd.Series(tokens)})\n",
    "        df.to_csv(f'{name_for_csv}.csv', index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1609459 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with google/flan-t5-xxl\n",
      "Done with bigscience/mt0-xxl-mt\n",
      "Done with CohereForAI/aya-101\n",
      "Done with bigscience/bloomz-7b1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1101189 > 131072). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with microsoft/Phi-3.5-mini-instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1129839 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with neulab/Pangea-7B\n",
      "Done with google/gemma-7b\n",
      "Done with google/gemma-2-9b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1097366 > 131072). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with meta-llama/Llama-3.2-1B-Instruct\n"
     ]
    }
   ],
   "source": [
    "models = [\"google/flan-t5-xxl\",\n",
    "          \"bigscience/mt0-xxl-mt\",\n",
    "          \"CohereForAI/aya-101\",\n",
    "          \"bigscience/bloomz-7b1\",\n",
    "          \"microsoft/Phi-3.5-mini-instruct\",\n",
    "          \"neulab/Pangea-7B\",\n",
    "          \"google/gemma-7b\",\n",
    "          \"google/gemma-2-9b\",\n",
    "          \"meta-llama/Llama-3.2-1B-Instruct\"]\n",
    "names_for_csv = ['flan-t5-xxl',\n",
    "                 'mt0-xxl-mt',\n",
    "                 'aya-101',\n",
    "                 'bloomz-7b1',\n",
    "                 'Phi-3.5-mini-instruct',\n",
    "                 'Pangea-7B',\n",
    "                 'gemma-7b',\n",
    "                 'gemma-2-9b', \n",
    "                 'Llama-3.2-1B-Instruct']\n",
    "\n",
    "for i in range(len(models)):\n",
    "    get_fertilities(model=models[i], name_for_csv=names_for_csv[i])\n",
    "    print(f'Done with {models[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>fertility</th>\n",
       "      <th>tokens</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cyrl</td>\n",
       "      <td>5.402301</td>\n",
       "      <td>['▁', 'Дүшә', 'м', 'б', 'е', '▁', 'С', 'т', 'э...</td>\n",
       "      <td>flan-t5-xxl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Khm</td>\n",
       "      <td>5.829346</td>\n",
       "      <td>['▁', 'កាល', '▁', 'ពី', '▁', 'ថ្ងៃ', '▁', 'ច័ន...</td>\n",
       "      <td>flan-t5-xxl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nk</td>\n",
       "      <td>2.077179</td>\n",
       "      <td>['▁', 'ߟߐ߲ߞߏߕߌ߮', '▁', 'ߟߎ߬', '▁', 'ߓߘߊ߫', '▁'...</td>\n",
       "      <td>flan-t5-xxl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ge</td>\n",
       "      <td>2.193156</td>\n",
       "      <td>['▁', 'ორშაბათს', ',', '▁', 'სტენფორდის', '▁',...</td>\n",
       "      <td>flan-t5-xxl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Heb</td>\n",
       "      <td>2.150794</td>\n",
       "      <td>['▁', 'ביום', '▁', 'שני', ',', '▁', 'מדענים', ...</td>\n",
       "      <td>flan-t5-xxl</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  language  fertility                                             tokens  \\\n",
       "0     Cyrl   5.402301  ['▁', 'Дүшә', 'м', 'б', 'е', '▁', 'С', 'т', 'э...   \n",
       "1      Khm   5.829346  ['▁', 'កាល', '▁', 'ពី', '▁', 'ថ្ងៃ', '▁', 'ច័ន...   \n",
       "2       Nk   2.077179  ['▁', 'ߟߐ߲ߞߏߕߌ߮', '▁', 'ߟߎ߬', '▁', 'ߓߘߊ߫', '▁'...   \n",
       "3       Ge   2.193156  ['▁', 'ორშაბათს', ',', '▁', 'სტენფორდის', '▁',...   \n",
       "4      Heb   2.150794  ['▁', 'ביום', '▁', 'שני', ',', '▁', 'מדענים', ...   \n",
       "\n",
       "         model  \n",
       "0  flan-t5-xxl  \n",
       "1  flan-t5-xxl  \n",
       "2  flan-t5-xxl  \n",
       "3  flan-t5-xxl  \n",
       "4  flan-t5-xxl  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make new dataset with columns ['model', 'language', 'fertility', 'tokens']\n",
    "# Basically combine everything done so far\n",
    "names_for_csv = ['flan-t5-xxl',\n",
    "                 'mt0-xxl-mt',\n",
    "                 'aya-101',\n",
    "                 'bloomz-7b1',\n",
    "                 'Phi-3.5-mini-instruct',\n",
    "                 'Pangea-7B',\n",
    "                 'gemma-7b',\n",
    "                 'gemma-2-9b', \n",
    "                 'Llama-3.2-1B-Instruct']\n",
    "\n",
    "dfs = []\n",
    "for file in os.listdir(os.getcwd()):\n",
    "    if file.rstrip('.csv') in names_for_csv: # Get correct files\n",
    "        df = pd.read_csv(file).drop(columns=['Unnamed: 0'])\n",
    "        df['model'] = file.rstrip('.csv')\n",
    "        dfs.append(df)\n",
    "\n",
    "\n",
    "merged_df = pd.concat(dfs, ignore_index=True)\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv('model_fertilities.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
